{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# Found table name using the following script\n",
    "# convention_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "# print(convention_cur.fetchall())\n",
    "# table: conventions\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text, party FROM conventions\n",
    "                            ''')\n",
    "\n",
    "for row in query_results :\n",
    "    # store the results in convention_data\n",
    "    \n",
    "    text, party = row\n",
    "    \n",
    "    # The  first element in the sublist should be the cleaned \n",
    "    # and tokenized text in a single string\n",
    "    \n",
    "    #********* Removing the punctuations ***************\n",
    "    punctuation = set(punctuation) # creating punctuation Set\n",
    "    more_punct_elements = [\"â€º\", \"Â«\", \"Ã—\"]\n",
    "    punctuation.update(more_punct_elements) # updating our punctuation sets\n",
    "    \n",
    "    \n",
    "    text_no_punct = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    \n",
    "    #********* Removing the numbers ***************\n",
    "    \n",
    "    text_no_numbers = \"\".join([ch for ch in text_no_punct if not ch.isdigit()])\n",
    "    \n",
    "    #********* Tokenizing ***************\n",
    "    whitespace_pattern = re.compile(r\"\\s+\")\n",
    "    \n",
    "    tokens = [item.lower() for item in whitespace_pattern.split(text_no_numbers)]\n",
    "    \n",
    "    #*********** Removing Stopwords **************\n",
    "    sw = stopwords.words(\"english\") # stopwords\n",
    "    \n",
    "    tokens_no_sw = [token for token in tokens if not token in sw]\n",
    "    \n",
    "    #********* From tokenized into  single string *************\n",
    "    \n",
    "    # tokenized text in a single string\n",
    "    \n",
    "    text_tokenized = \" \".join(tokens_no_sw)\n",
    "    \n",
    "    #********* Appending to convention_data ***************\n",
    "    # The second element should be the party\n",
    "    \n",
    "    convention_data.append([text_tokenized, party])\n",
    "    \n",
    "# print(convention_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['get meaningful legislation congress march south streets jackson streets danville streets cambridge streets birmingham james lawson jr think singular figure tried carry work nonviolent campaigns halls congress',\n",
       "  'Democratic'],\n",
       " ['well confess expected speaking tonight iâ€™m mile away one wildfires weâ€™re battling state california coming record week heat wave led degree temperatures highest temperature ever recorded california arguably worldâ€™s history state hots getting hotter dries getting drier climate change real denial climate change come california dry lightening strikes hour period leading unprecedented challenge wildfires',\n",
       "  'Democratic'],\n",
       " ['let give detail early april president trump organized task force business leaders across country fortunate enough serve task force firsthand opportunity witness hardworking determined president trump solve unprecedented problems country facing personally observed ability listen understand issues impacting americans backgrounds clear highest priority always health safety everyone country republicans supporters every single american',\n",
       "  'Republican'],\n",
       " ['need prove world importantly prove better', 'Democratic'],\n",
       " ['iâ€™ve served two republican presidents one democratic president seen trump administration make decisions without thought without preparation massive life death consequences',\n",
       "  'Democratic'],\n",
       " ['mission fight future equal ideals founders hopes children sacrifices veterans brave men women uniform families',\n",
       "  'Democratic'],\n",
       " ['need leaders equal moment sacrifice service need joe biden kamala harris',\n",
       "  'Democratic'],\n",
       " ['good', 'Republican'],\n",
       " ['thank much', 'Republican'],\n",
       " ['harnessing resources federal government private sector president trumpâ€™s operation warp speed accelerating testing supply development distribution therapeutics diagnostics shortly effective vaccines counter covid let give three clear examples president trumpâ€™s leadership removed regulatory barriers covid patients could faster access effective therapies diagnostics first february two phase three clinical trials studying remdesivir initiated two months later fda approved remdesivir emergency use treat covid normally three five year process amazing speed happened safe efficient manner unheard second fda granted expanded access covid convalescent plasma within hours approval administering convalescent plasma remdesivir critically ill patient former army ranger physician dr ge',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2338 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    ret_dict = dict()\n",
    "    \n",
    "    for i in text.split():\n",
    "    \n",
    "        if i in fw:\n",
    "\n",
    "            ret_dict[i] = True\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.496\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     25.8 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     14.9 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                  defund = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "china, votes, destroy, freedoms are use so often by the republican calss. Similarly beliefs, isis, religion, countries is used by republican class but not even once by deomocratic class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Brooks Joins Alabama Delegation in Voting Against Flawed Funding Bill\" http://t.co/3CwjIWYsNq'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text_example = results[0][2]\n",
    "tweet_text_example.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "\n",
    "for row in results:\n",
    "    # store the results in convention_data\n",
    "    \n",
    "    candidate, party, text = row\n",
    "    \n",
    "    # The  first element in the sublist should be the cleaned \n",
    "    # and tokenized text in a single string\n",
    "    \n",
    "    #************* Decoding the Bytes ******************\n",
    "    text_decoded = text.decode('utf-8')\n",
    "    \n",
    "    #********* Removing the punctuations ***************\n",
    "    punctuation = set(punctuation) # creating punctuation Set\n",
    "    more_punct_elements = [\"â€º\", \"Â«\", \"Ã—\"]\n",
    "    punctuation.update(more_punct_elements) # updating our punctuation sets\n",
    "    \n",
    "    \n",
    "    text_no_punct = \"\".join([ch for ch in text_decoded if ch not in punctuation])\n",
    "    \n",
    "    #********* Removing the numbers ***************\n",
    "    \n",
    "    text_no_numbers = \"\".join([ch for ch in text_no_punct if not ch.isdigit()])\n",
    "    \n",
    "    #********* Tokenizing ***************\n",
    "    whitespace_pattern = re.compile(r\"\\s+\")\n",
    "    \n",
    "    tokens = [item.lower() for item in whitespace_pattern.split(text_no_numbers)]\n",
    "    \n",
    "    #*********** Removing Stopwords **************\n",
    "    sw = stopwords.words(\"english\") # stopwords\n",
    "    \n",
    "    tokens_no_sw = [token for token in tokens if not token in sw]\n",
    "    \n",
    "    #********* From tokenized into  single string *************\n",
    "    \n",
    "    # tokenized text in a single string\n",
    "    \n",
    "    text_tokenized = \" \".join(tokens_no_sw)\n",
    "    \n",
    "    #********* Appending to convention_data ***************\n",
    "    # The second element should be the party\n",
    "    \n",
    "    tweet_data.append([text_tokenized, party])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast httpstcowqgtrztvv\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: go tribe rallytogether httpstconxutfll\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparently trump thinks easy students overwhelmed crushing burden debt pay student loans trumpbudget httpstcockyqotqh\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: weâ€™re grateful first responders rescue personnel firefighters police volunteers working tirelessly keep people safe provide muchneeded help putting lives line httpstcoezpvvmiz\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: letâ€™s make even greater kag ðŸ‡ºðŸ‡¸ httpstcoyqozdlz\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: hr cavs tie series im allin repbarbaralee scared roadtovictory\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats belliottsd new gig sd city hall glad continue serveâ€¦ httpstcofkvmwcqdi\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: really close raised toward match right whoot thatâ€™s nonmath majors room ðŸ˜‚ help us get httpstcotucsd httpstcoqsdqkypsmc\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: today comment period potusâ€™s plan expand offshore drilling opened public days march share oppose proposed program directly trump administration comments made email mail httpstcobaaymejxqn\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated icseastlaâ€™s years eastside commitment amp saluted community leaders last nightâ€™s awards dinner httpstcovghgivb\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet, party in tweet_data_sample :\n",
    "    \n",
    "    featureset = conv_features(tweet, feature_words)\n",
    "    \n",
    "    estimated_party = classifier.classify(featureset)\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "    featureset = conv_features(tweet, feature_words)\n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = classifier.classify(featureset)\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3722, 'Democratic': 556}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4844, 'Democratic': 880})})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "Out of the 10,000 tweets:\n",
    "\n",
    "3722 + 556  = 4,278 are actually \"Republican\":\n",
    "    . Out of those 4,278 , our classifier estimated 3722/4278 = 0.87 ~ 87% accurately \"Republican\"\n",
    "\n",
    "4844 + 880  = 5,724 are actually \"Democratic\":\n",
    "    . Out of those 5,724 , our classifier estimated 880/5724 =  0.15 ~ 15% accurately \"Democratic\"\n",
    "    \n",
    "Conclusion: our classifier did not have another label to train on (did not use \"False\"), therefore with a accuracy of approx. 50% our classification results for the tweet data are not surprising, with the classifier only doing good with one of the labels \"Republican\" and really badly with the other label \"Democratic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
